{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZJDPs2t4pCS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import string, os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from math import log\n",
        "from google.colab import drive\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_distributed_training():\n",
        "    \"\"\"\n",
        "    Set up distributed training strategy for TPU or default strategy for CPU/GPU.\n",
        "\n",
        "    Returns:\n",
        "        tf.distribute.Strategy: The distributed training strategy.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if TPU is available and configure the strategy accordingly\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Device:', tpu.master())\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "        print('Using TPU strategy.')\n",
        "    except ValueError:\n",
        "        # TPU is not available, fall back to CPU/GPU strategy\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        print('Using default strategy for CPU/GPU.')\n",
        "\n",
        "    print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "    return strategy\n",
        "\n",
        "# Set up the distributed training strategy\n",
        "strategy = setup_distributed_training()\n",
        "\n",
        "# Set AUTOTUNE for optimizing performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "metadata": {
        "id": "mNpxbSGv40Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "B1Itk0hq42_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/gdrive/MyDrive/topical_chat.csv /content/"
      ],
      "metadata": {
        "id": "TU9R02Y94404"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 200\n",
        "latent_dim = 512"
      ],
      "metadata": {
        "id": "6vug2eAk46tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('/content/topical_chat.csv')[:160000]\n",
        "\n",
        "    print('Length of dataset:', len(df))\n",
        "\n",
        "    # Convert the 'message' column to string type\n",
        "    df['message'] = df['message'].astype(str)\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    df.head()\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset file not found.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"The dataset file is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error occurred while parsing the dataset.\")"
      ],
      "metadata": {
        "id": "4pbDf1qC49BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase and replace specific characters\n",
        "    text = text.lower().replace('\\n', ' ').replace('-', ' ').replace(':', ' ').replace(',', '') \\\n",
        "          .replace('\"', ' ').replace(\".\", \" \").replace(\"!\", \" \").replace(\"?\", \" \").replace(\";\", \" \").replace(\":\", \" \")\n",
        "\n",
        "    # Remove punctuation and extra spaces\n",
        "    text = ''.join(char for char in text if char not in string.punctuation)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aYV168Za4_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.message = df.message.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "9CJfbnVT5CuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "TuQwYHHg5GyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# Initialize sets to store unique words\n",
        "input_words_set = set()\n",
        "target_words_set = set()\n",
        "\n",
        "# Iterate through the dataset to process conversations\n",
        "for conversation_index in tqdm(range(1, len(df))):\n",
        "    # Get the current and previous rows (messages in a conversation)\n",
        "    current_row = df.iloc[conversation_index]\n",
        "    previous_row = df.iloc[conversation_index - 1]\n",
        "\n",
        "    # Check if the conversation IDs match\n",
        "    if current_row['conversation_id'] == previous_row['conversation_id']:\n",
        "        input_text = previous_row['message']\n",
        "        target_text = current_row['message']\n",
        "\n",
        "        # Check conditions for valid input and target texts\n",
        "        if (2 < len(input_text.split()) < 30 and\n",
        "            0 < len(target_text.split()) < 10 and\n",
        "            input_text and target_text):\n",
        "\n",
        "            # Add start and end tokens to the target text\n",
        "            target_text = \"bos \" + target_text + \" eos\"\n",
        "\n",
        "            # Append to input and target text lists\n",
        "            input_texts.append(input_text)\n",
        "            target_texts.append(target_text)\n",
        "\n",
        "            # Update the sets of unique words\n",
        "            input_words_set.update(input_text.split())\n",
        "            target_words_set.update(target_text.split())\n",
        "\n",
        "print(\"\\nNumber of unique input words:\", len(input_words_set))\n",
        "print(\"Number of unique target words:\", len(target_words_set))"
      ],
      "metadata": {
        "id": "CT67he3f5IEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_words = sorted(list(input_words_set))\n",
        "target_words = sorted(list(target_words_set))\n",
        "\n",
        "# Calculate various lengths and quantities\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max(len(txt.split()) for txt in input_texts)\n",
        "max_decoder_seq_length = max(len(txt.split()) for txt in target_texts)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# Create token index dictionaries\n",
        "input_token_index = {word: i for i, word in enumerate(input_words)}\n",
        "target_token_index = {word: i for i, word in enumerate(target_words)}\n",
        "\n",
        "# Initialize arrays for encoder and decoder data\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Populate the arrays with token indices\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t] = input_token_index[word]\n",
        "\n",
        "    for t, word in enumerate(target_text.split()):\n",
        "        decoder_input_data[i, t] = target_token_index[word]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[word]] = 1.0\n",
        "\n",
        "print(\"Encoder Input Data shape:\", encoder_input_data.shape)\n",
        "print(\"Decoder Input Data shape:\", decoder_input_data.shape)\n",
        "print(\"Decoder Target Data shape:\", decoder_target_data.shape)"
      ],
      "metadata": {
        "id": "tBY0MxQR5LDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100\n",
        "\n",
        "# Define encoder\n",
        "with strategy.scope():\n",
        "    encoder_inputs = keras.Input(shape=(None,))\n",
        "    encoder_embedding_output = keras.layers.Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
        "    encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define decoder\n",
        "with strategy.scope():\n",
        "    decoder_inputs = keras.Input(shape=(None,))\n",
        "    decoder_embedding = keras.layers.Embedding(num_decoder_tokens, embedding_size)  # Define decoder_embedding here\n",
        "    decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "with strategy.scope():\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "# Training\n",
        "with strategy.scope():\n",
        "    history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=30, validation_split=0.1)\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()\n",
        "\n",
        "# Define the decoder model\n",
        "with strategy.scope():\n",
        "    decoder_state_input_h = keras.Input(shape=(None,))\n",
        "    decoder_state_input_c = keras.Input(shape=(None,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n"
      ],
      "metadata": {
        "id": "TKZ3UInd5N3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_token_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "9htE6kFF5Q2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(input_text, max_encoder_seq_length, input_token_index,\n",
        "                      target_token_index, encoder_model, decoder_model,\n",
        "                      reverse_target_token_index, max_response_length=50):\n",
        "    \"\"\"\n",
        "    Generate a response based on the input text.\n",
        "    Args:\n",
        "        input_text (str): The input text to generate a response for.\n",
        "        max_encoder_seq_length (int): Maximum length of the encoder sequence.\n",
        "        input_token_index (dict): Dictionary mapping input words(tokens) to indices.\n",
        "        target_token_index (dict): Dictionary mapping target words(tokens) to indices.\n",
        "        encoder_model: The encoder model.\n",
        "        decoder_model: The decoder model.\n",
        "        reverse_target_token_index (dict): Dictionary mapping indices to target words(tokens).\n",
        "        max_response_length (int): Maximum length of the generated response.\n",
        "    Returns:\n",
        "        str: The generated response.\n",
        "    \"\"\"\n",
        "    input_seq = np.zeros((1, max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    # Tokenize the input text and create the input sequence\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        # Convert words to their respective token indices\n",
        "        input_seq[0, t] = input_token_index[word]\n",
        "\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Initialize the target sequence with the start character\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_token_index['bos']  # 'bos' indicates the start of a sentence\n",
        "\n",
        "    # Sampling loop to generate the response\n",
        "    generated_sentence = ''\n",
        "    for _ in range(max_response_length):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        # Sample a token by selecting the one with the highest probability (greedy method)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_token_index[sampled_token_index]\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop character\n",
        "        if sampled_token == 'eos' or len(generated_sentence) > max_response_length:\n",
        "            break\n",
        "        else:\n",
        "            generated_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Update the target sequence (of length 1) with the sampled token\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update the states for the next iteration\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return generated_sentence\n"
      ],
      "metadata": {
        "id": "Xl5Scgsl5TTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequences_to_generate = 10\n",
        "\n",
        "for seq_index in range(num_sequences_to_generate):\n",
        "    input_text = input_texts[seq_index]\n",
        "    generated_sentence = generate_text(input_text, max_encoder_seq_length,\n",
        "                                        input_token_index, target_token_index,\n",
        "                                        encoder_model, decoder_model,\n",
        "                                        reverse_target_token_index)\n",
        "\n",
        "    print(\"input sentence:\", input_text)\n",
        "    print(\"generated sentence:\", generated_sentence)"
      ],
      "metadata": {
        "id": "Sk6ks6pQ5Wr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Generate text using beam search.\n",
        "    Args:\n",
        "            input_text (str): The initial text to start the generation.\n",
        "            number_of_words (int): The number of words to generate.\n",
        "            beam_search_n (int): Beam width for the search.\n",
        "            max_encoder_seq_length (int): Maximum length of the encoder sequence.\n",
        "            input_token_index (dict): Dictionary mapping input words(tokens) to indices.\n",
        "            target_token_index (dict): Dictionary mapping target words(tokens) to indices.\n",
        "            encoder_model: The encoder model.\n",
        "            decoder_model: The decoder model.\n",
        "            reverse_target_token_index (dict): Dictionary mapping indices to target_words.\n",
        "    Returns:\n",
        "            None\n",
        "    \"\"\"\n",
        "\n",
        "import heapq\n",
        "\n",
        "# define class Sequence:\n",
        "\n",
        "class Sequence:\n",
        "    def __init__(self, token_indices, hidden_state, cell_state, log_prob):\n",
        "        self.token_indices = token_indices\n",
        "        self.hidden = hidden_state\n",
        "        self.cell = cell_state\n",
        "        self.log_prob = log_prob\n",
        "\n",
        "    def add_token(self, token_index, h, c, log_prob):\n",
        "        new_token_indices = self.token_indices + [token_index]\n",
        "        return Sequence(new_token_indices, h, c, self.log_prob + log_prob)\n",
        "\n",
        "    def get_last_token(self):\n",
        "        return self.token_indices[-1]\n",
        "\n",
        "    def get_total_log_prob(self):\n",
        "        return self.log_prob\n",
        "\n",
        "def generate_text_with_beam_search(input_text, number_of_words, beam_search_n,\n",
        "                                   max_encoder_seq_length, input_token_index, target_token_index,\n",
        "                                   encoder_model, decoder_model, reverse_target_token_index):\n",
        "\n",
        "\n",
        "    input_seq = np.zeros((1, max_encoder_seq_length), dtype=\"float32\")\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        input_seq[0, t] = input_token_index[word]\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_token_index['bos']\n",
        "\n",
        "    initial_token_seq = Sequence([target_token_index['bos']], *states_value, 0)\n",
        "    token_seqs = [initial_token_seq]\n",
        "\n",
        "    for _ in range(number_of_words):\n",
        "        new_token_seqs = []\n",
        "        for token_seq in token_seqs:\n",
        "            target_seq[0, 0] = token_seq.get_last_token()\n",
        "            output_tokens, hidden, cell = decoder_model.predict([target_seq] + [token_seq.hidden, token_seq.cell])\n",
        "            top_indices = heapq.nlargest(beam_search_n, range(len(output_tokens[0, -1, :])), key=output_tokens[0, -1, :].__getitem__)\n",
        "            for index in top_indices:\n",
        "                log_prob = np.log(output_tokens[0, -1, index])\n",
        "                new_token_seq = token_seq.add_token(index, hidden, cell, log_prob)\n",
        "                new_token_seqs.append(new_token_seq)\n",
        "        new_token_seqs = heapq.nlargest(beam_search_n, new_token_seqs, key=Sequence.get_total_log_prob)\n",
        "        token_seqs = new_token_seqs\n",
        "\n",
        "    response_dict = {}\n",
        "    for token_seq in token_seqs:\n",
        "        response = ' '.join(reverse_target_token_index[token] for token in token_seq.token_indices[1:])\n",
        "        response_dict[response] = np.exp(token_seq.get_total_log_prob())\n",
        "\n",
        "    for response, probability in response_dict.items():\n",
        "        print(\"response:\", response, \", probability:\", probability)"
      ],
      "metadata": {
        "id": "V0r2rQiL5Y7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequences_to_generate = 5\n",
        "number_of_words = 5\n",
        "beam_search_n = 2\n",
        "\n",
        "for seq_index in range(num_sequences_to_generate):\n",
        "    input_text = input_texts[seq_index]\n",
        "    print(\"input sentence:\", input_text)\n",
        "    generated_sentence = generate_text_with_beam_search(input_text, number_of_words, beam_search_n,\n",
        "                        max_encoder_seq_length, input_token_index, target_token_index,\n",
        "                        encoder_model, decoder_model, reverse_target_token_index)"
      ],
      "metadata": {
        "id": "0d7PLnQj5bfc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}